{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "raw_df = pd.read_csv('SpotifyFeatures.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def wrangle(df):\n",
    "    # drop duplicate entries\n",
    "    df = df.drop_duplicates(subset='track_id')\n",
    "        \n",
    "    # drop identifier column\n",
    "    df = df.drop(columns='track_id')\n",
    "\n",
    "    # remove '#' from the 'key'column\n",
    "    df['key'] = df['key'].str.replace('#', '')\n",
    "\n",
    "    # reindex dataframe (some indexes were lost due to dropping duplicate rows)\n",
    "    df = df.reset_index()\n",
    "    # drop the additional 'index' column which was created when reindexing\n",
    "    df.drop(columns='index', inplace=True)\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df = wrangle(raw_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python3 -m spacy download en_core_web_md"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    Use the pre-trained model from Space to tokenize our text into lemmas \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Rememeber that the pre-trained spaCy model has a lot of built in flags for what kind of token each token is\n",
    "    so we can use that functionality to create filters for stop words, white spaces, punctuation, and so on!\n",
    "    \n",
    "    See list of flags here: https://spacy.io/api/token#attributes\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    text: string\n",
    "        Full text article/document that needs to be tokenized \n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for token in nlp(text):\n",
    "        # if statement will filter out stopwords, punctuation, and whitespace\n",
    "        # COMPLETE THE CODE HERE\n",
    "        if (token.is_stop != True) & (token.is_punct != True ) & (token.is_space != True) & (token.is_digit != True):\n",
    "            # Now lemmatize!\n",
    "            tokens.append(token.lemma_)\n",
    "    \n",
    "    return ' '.join(tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df['lemmas_track_name'] = df['track_name'].apply(tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def vectorize(text):\n",
    "    \"\"\"Vectorize the tokenized text\"\"\"\n",
    "    return nlp(text).vector"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "df['vec_track_name'] = df['track_name'].apply(vectorize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.to_csv('tokenized-vectorized-df')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.to_csv(\"tokenized-vectorized-compr-df.csv.zip\", \n",
    "           index=False, \n",
    "           compression=\"zip\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = df.drop(columns=['artist_name', 'track_name'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from category_encoders import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "enc = OrdinalEncoder()\n",
    "df_encoded = enc.fit_transform(df1)\n",
    "df_encoded.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_encoded.values), columns=df1.columns)\n",
    "df_scaled.head()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('SpotifyTest11-wvY6x2gN': pipenv)"
  },
  "interpreter": {
   "hash": "82dbf13e8cfb89b4dd6d6ddd4150e6b93671c28f1c3e0c9a17a4083d61c76f6b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}